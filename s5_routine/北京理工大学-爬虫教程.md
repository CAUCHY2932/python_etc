
# scrapy教程

## requests和scrapy的比较

## scrapy爬虫框架的结构
5+2模式engine和scheduler，以及downloader都不需要修改
### spider
解析downloader返回的响应response
产生爬取项item
产生额外的爬取请求request
### item pipelines
以流水线方式处理spider产生的爬取项
由一组操作顺序组成，类似流水线，每个操作是一个item pipeline类型
可能操作包括：清理检验和查重，爬取项中的html数据，将数据存储到数据库中（需要用户编写配置代码）
### spider middleware
目的：对请求和爬取项的再处理
功能：修改、丢弃、新增请求或爬取项（用户编写配置代码）我们发现，5+2结构中，许多模块已经是既定的，我们重点编写的是spider模块和item pipeline模块Scrapy是用Python开发的一个开源的Web爬虫框架,可用于快速抓取Web站点并从页面中高效提取结构化的数据。Scrapy可广泛应用于数据挖掘、信息处理或存储历史数据等一系列的程序中。提供了多种类型爬虫的基类，如BaseSpider、SitemapSpider等。其最初是为了页面抓取(更确切来说,网络抓取)所设计的，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services)或者通用的网络爬虫。
  Scrapy基于事件驱动网络框架 Twisted 编写,因而它采用的是基于事件驱动的设计,这种设计模型有利于与使用者进行交互,由于Scrapy并发性的考虑,因而被设计成非阻塞(异步)的实现。
  Scrapy爬虫框架共包括7各部分，称之为“5+2”结构。主体结构包括Scrapy Engine、Spiders、Downloader、Item Pipelines和Scheduler模块，同时在Engine和Downloader、Spiders之间有两个中间件模块,分别是Downloader Middleware和Spider Middleware模块。
### scrapy安装
conda install scrapy
* scrapy 是一个爬虫框架

    * 爬虫框架是实现爬虫功能的一个软件结构和功能组件集合
    * 爬虫框架是一个半成品，能够帮助用户实现专业网络爬虫

### 5+2结构
主体：
engine
item pipelines
downloader
spider
scheduler

engine和spider之间有一个中间件
engine和downloader之间有一个中间件


![scrapy结构图](img/scrapy爬虫框架结构图.jpg)

### 插入图片的方法：
    * 单元格改成，md格式，然后按照下图修改
![scrapy结构图](img/scrapy爬虫框架结构图.jpg)5个主体中只有spider和item pipeline需要用户配置
spider用来提供整个框架需要访问的url链接，同时解析页面上要获得的内容
item pipeline模块需要对提取的信息进行后处理这种编写方式我们称之为配置
## scrapy爬虫的第一个实例
演示地址：http://python123.io/ws/demo.html文件名称：demo.html
### 产生一个爬虫框架
步骤一：建立一个scrapy爬虫工程选取一个目录，使用命令行创建工程
scrapy stratproject python123demo它会生成一个目录，
python123demo/ 外层目录
    scrapy.cfg  部署scrapy爬虫的配置文件，放在特定的服务器上，运行的配置文件
    python123demo/  scrapy框架的用户自定义python代码
        __init__.py  初始化脚本，不需要用户编写
        items.py    items代码模板（继承类），不需要用户编写
        middlewares.py     middlewares代码模板（继承类），如果需要扩展，用户可以自行编写
        pipelines.py      pipelines代码模板（继承类）
        settings.py       scrapy爬虫的配置文件，如果我们需要优化性能，就需要修改这个文件
        spiders/        spiders代码模板目录（继承类）
        
        
        
spiders/  spiders代码模板目录（继承类）
    __init__.py   初始文件，无需修改
    __pycache__/  缓存目录，无需修改步骤二：在工程中产生一个scrapy爬虫切换到工程根目录下：如pro/
打开命令行
需要指定爬虫名和访问网址的主机
scrapy genspider demo python123.io在spiders/ 下生成了文件
我们也可以手动生成文件

```python
# -*- coding: utf-8 -*-
import scrapy


class FirstSpiderSpider(scrapy.Spider):
    name = 'first_spider'
    allowed_domains = ['www.jd.com/']
    start_urls = ['http://www.jd.com//']

    def parse(self, response):
        pass

```
对于demo.py的内容中，类名不重要，但是一定要继承于scrapy.spider
name，当前爬虫名
allowed_domains，指定我们只能爬取这个域名以下的页面
start_urls，指定我们爬取的初始页面
parse方法，解析页面
用于处理响应，解析内容形成字典，发现新的url爬取请求步骤三：配置产生的spider爬虫我们修改demo.py文件，使其满足我们的要求打开demo.py文件
allowed_domains 这一行可以注释
修改start_urls为第一次需要访问的链接

我们需要把访问的内容写入到html文件中
# -*- coding: utf-8 -*-

import scrapy


class DemoSpider(scrapy.Spider):
    name = 'demo'
    # allowed_domains = ['python123.io']
    start_urls = ['http://python123.io/ws/demo.html']

    def parse(self, response):
        fileName=response.url.split('/')[-1]
        with open(fileName,'wb')as f:
            f.write(response.body)
        self.log('save file {}'.format(fileName))
        pass
步骤四：运行爬虫，获取网页在工程根目录下打开命令行下
如python123demo/
scrapy crawl demo
这里demo指的是爬虫名爬虫运行结束我们可以发现，demo.html存储到了根目录中实际上，spiders有一个完整版写法
start_urls
被定义为一个类方法start_requests(),然后对urls列表中的每一个url进行request请求，yield返回结果
### scrapy爬虫的使用步骤
步骤一：创建一个工程和spider模板
步骤二：编写spider
步骤三：编写item pipeline
步骤四：优化配置策略scrapy爬虫的数据类型Request类
代表向网络提交请求的内容Response类
从网络中爬取内容的封装类Item类
从spider产生的信息封装的类
#### Request类
class scrapy.http.Request()
Request对象表示一个HTTP请求
由spider生成，由downloader执行包含六个常用的属性和方法
.url Request 对应的请求url地址
.method Request对应的请求方法，'GET','POST'
.headers 字典类型风格的请求头
.body 请求内容主体，字符串类型
.meta 用户添加的扩展信息，在scrapy内部模板间传递信息使用
.copy 复制该请求
#### Response类
class scrapy.http.Response()
Response对象表示一个HTTP响应
由Downloader生成，由spider来处理包含七个常用的属性和方法
.url Response对应的返回信息所关联的url地址
.status HTTP状态码，默认是200
.headers Response对应的头部信息
.body Response对应的内容信息，字符串类型
.flags Response对应的一组标记
.request 产生Response类型对应的Request对象
.copy() 复制该响应方法scrapy和requests库的方法相似，这是因为他们都是对HTTP协议做处理和响应
HTTP请求的相应字段是固定的
#### Item类
class scrapy.item.Item()
item对象表示一个从HTML页面中提取的信息内容
由spider生成，由item pipeline处理
Item类似字典类型，可以按照字典类型操作在scrapy框架下，item类是以字典进行定义的
spider对相关网页提取后，把其中提取的信息生成键值对，并封装成字典，这种字典就是Item类
### scrapy爬虫提取信息的方法
scrapy爬虫支持多种HTML信息提取方法

Beautifulsoup
lxml
re
Xpath Selector
CSS Selector
#### CSS Selector的基本使用
<HTML>.css('a::attr(href)').extract()
标签名 标签属性
CSS Selector由W3C组织维护并规范
### yield关键字
python3中33个关键字中比较重要的一个
yield->生成器生成器是一个不断产生值的函数
包含yield语句的函数就是一个生成器
生成器每次产生一个值（yield语句），函数被冻结，被唤醒后再产生一个值
唤醒之后，所使用的局部变量的值和之前所使用的值是一致的
##### 生成器实例


```python
def gen(n):
    for i in range(n):
        yield i**2
```


```python
for i in gen(5):
    print(i,' ',end='')
```

    0  1  4  9  16  

##### 普通写法


```python
def square(n):
    ls=[i**2 for i in range(n)]
    return ls
```


```python
for i in square(5):
    print(i,' ',end='')
```

    0  1  4  9  16  

#### 为何要有生成器

##### 生成器相比一次列出所有内容的优势


```python
更节省存储空间
响应更迅速
使用更加灵活
```
