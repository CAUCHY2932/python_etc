{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy教程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests和scrapy的比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrapy爬虫框架的结构"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5+2模式"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "engine和scheduler，以及downloader都不需要修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spider"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "解析downloader返回的响应response\n",
    "产生爬取项item\n",
    "产生额外的爬取请求request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item pipelines"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "以流水线方式处理spider产生的爬取项\n",
    "由一组操作顺序组成，类似流水线，每个操作是一个item pipeline类型\n",
    "可能操作包括：清理检验和查重，爬取项中的html数据，将数据存储到数据库中（需要用户编写配置代码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spider middleware"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "目的：对请求和爬取项的再处理\n",
    "功能：修改、丢弃、新增请求或爬取项（用户编写配置代码）"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们发现，5+2结构中，许多模块已经是既定的，我们重点编写的是spider模块和item pipeline模块"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scrapy是用Python开发的一个开源的Web爬虫框架,可用于快速抓取Web站点并从页面中高效提取结构化的数据。Scrapy可广泛应用于数据挖掘、信息处理或存储历史数据等一系列的程序中。提供了多种类型爬虫的基类，如BaseSpider、SitemapSpider等。其最初是为了页面抓取(更确切来说,网络抓取)所设计的，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services)或者通用的网络爬虫。\n",
    "  Scrapy基于事件驱动网络框架 Twisted 编写,因而它采用的是基于事件驱动的设计,这种设计模型有利于与使用者进行交互,由于Scrapy并发性的考虑,因而被设计成非阻塞(异步)的实现。\n",
    "  Scrapy爬虫框架共包括7各部分，称之为“5+2”结构。主体结构包括Scrapy Engine、Spiders、Downloader、Item Pipelines和Scheduler模块，同时在Engine和Downloader、Spiders之间有两个中间件模块,分别是Downloader Middleware和Spider Middleware模块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapy安装"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scrapy 是一个爬虫框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * 爬虫框架是实现爬虫功能的一个软件结构和功能组件集合\n",
    "    * 爬虫框架是一个半成品，能够帮助用户实现专业网络爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5+2结构"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "主体：\n",
    "engine\n",
    "item pipelines\n",
    "downloader\n",
    "spider\n",
    "scheduler\n",
    "\n",
    "engine和spider之间有一个中间件\n",
    "engine和downloader之间有一个中间件\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scrapy结构图](img/scrapy爬虫框架结构图.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 插入图片的方法：\n",
    "    * 单元格改成，md格式，然后按照下图修改"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "![scrapy结构图](img/scrapy爬虫框架结构图.jpg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5个主体中只有spider和item pipeline需要用户配置\n",
    "spider用来提供整个框架需要访问的url链接，同时解析页面上要获得的内容\n",
    "item pipeline模块需要对提取的信息进行后处理"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "这种编写方式我们称之为配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrapy爬虫的第一个实例"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "演示地址："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://python123.io/ws/demo.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "文件名称：demo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 产生一个爬虫框架"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "步骤一：建立一个scrapy爬虫工程"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "选取一个目录，使用命令行创建工程\n",
    "scrapy stratproject python123demo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "它会生成一个目录，\n",
    "python123demo/ 外层目录\n",
    "    scrapy.cfg  部署scrapy爬虫的配置文件，放在特定的服务器上，运行的配置文件\n",
    "    python123demo/  scrapy框架的用户自定义python代码\n",
    "        __init__.py  初始化脚本，不需要用户编写\n",
    "        items.py    items代码模板（继承类），不需要用户编写\n",
    "        middlewares.py     middlewares代码模板（继承类），如果需要扩展，用户可以自行编写\n",
    "        pipelines.py      pipelines代码模板（继承类）\n",
    "        settings.py       scrapy爬虫的配置文件，如果我们需要优化性能，就需要修改这个文件\n",
    "        spiders/        spiders代码模板目录（继承类）\n",
    "        \n",
    "        \n",
    "        \n",
    "spiders/  spiders代码模板目录（继承类）\n",
    "    __init__.py   初始文件，无需修改\n",
    "    __pycache__/  缓存目录，无需修改"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "步骤二：在工程中产生一个scrapy爬虫"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "切换到工程根目录下：如pro/\n",
    "打开命令行\n",
    "需要指定爬虫名和访问网址的主机\n",
    "scrapy genspider demo python123.io"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在spiders/ 下生成了文件\n",
    "我们也可以手动生成文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class FirstSpiderSpider(scrapy.Spider):\n",
    "    name = 'first_spider'\n",
    "    allowed_domains = ['www.jd.com/']\n",
    "    start_urls = ['http://www.jd.com//']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "对于demo.py的内容中，类名不重要，但是一定要继承于scrapy.spider\n",
    "name，当前爬虫名\n",
    "allowed_domains，指定我们只能爬取这个域名以下的页面\n",
    "start_urls，指定我们爬取的初始页面\n",
    "parse方法，解析页面\n",
    "用于处理响应，解析内容形成字典，发现新的url爬取请求"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "步骤三：配置产生的spider爬虫"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们修改demo.py文件，使其满足我们的要求"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "打开demo.py文件\n",
    "allowed_domains 这一行可以注释\n",
    "修改start_urls为第一次需要访问的链接\n",
    "\n",
    "我们需要把访问的内容写入到html文件中\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class DemoSpider(scrapy.Spider):\n",
    "    name = 'demo'\n",
    "    # allowed_domains = ['python123.io']\n",
    "    start_urls = ['http://python123.io/ws/demo.html']\n",
    "\n",
    "    def parse(self, response):\n",
    "        fileName=response.url.split('/')[-1]\n",
    "        with open(fileName,'wb')as f:\n",
    "            f.write(response.body)\n",
    "        self.log('save file {}'.format(fileName))\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "步骤四：运行爬虫，获取网页"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在工程根目录下打开命令行下\n",
    "如python123demo/\n",
    "scrapy crawl demo\n",
    "这里demo指的是爬虫名"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "爬虫运行结束我们可以发现，demo.html存储到了根目录中"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "实际上，spiders有一个完整版写法\n",
    "start_urls\n",
    "被定义为一个类方法start_requests(),然后对urls列表中的每一个url进行request请求，yield返回结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapy爬虫的使用步骤"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "步骤一：创建一个工程和spider模板\n",
    "步骤二：编写spider\n",
    "步骤三：编写item pipeline\n",
    "步骤四：优化配置策略"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy爬虫的数据类型"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Request类\n",
    "代表向网络提交请求的内容"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Response类\n",
    "从网络中爬取内容的封装类"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Item类\n",
    "从spider产生的信息封装的类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request类"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class scrapy.http.Request()\n",
    "Request对象表示一个HTTP请求\n",
    "由spider生成，由downloader执行"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "包含六个常用的属性和方法\n",
    ".url Request 对应的请求url地址\n",
    ".method Request对应的请求方法，'GET','POST'\n",
    ".headers 字典类型风格的请求头\n",
    ".body 请求内容主体，字符串类型\n",
    ".meta 用户添加的扩展信息，在scrapy内部模板间传递信息使用\n",
    ".copy 复制该请求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response类"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class scrapy.http.Response()\n",
    "Response对象表示一个HTTP响应\n",
    "由Downloader生成，由spider来处理"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "包含七个常用的属性和方法\n",
    ".url Response对应的返回信息所关联的url地址\n",
    ".status HTTP状态码，默认是200\n",
    ".headers Response对应的头部信息\n",
    ".body Response对应的内容信息，字符串类型\n",
    ".flags Response对应的一组标记\n",
    ".request 产生Response类型对应的Request对象\n",
    ".copy() 复制该响应方法"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy和requests库的方法相似，这是因为他们都是对HTTP协议做处理和响应\n",
    "HTTP请求的相应字段是固定的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item类"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class scrapy.item.Item()\n",
    "item对象表示一个从HTML页面中提取的信息内容\n",
    "由spider生成，由item pipeline处理\n",
    "Item类似字典类型，可以按照字典类型操作"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在scrapy框架下，item类是以字典进行定义的\n",
    "spider对相关网页提取后，把其中提取的信息生成键值对，并封装成字典，这种字典就是Item类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapy爬虫提取信息的方法"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy爬虫支持多种HTML信息提取方法\n",
    "\n",
    "Beautifulsoup\n",
    "lxml\n",
    "re\n",
    "Xpath Selector\n",
    "CSS Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSS Selector的基本使用"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<HTML>.css('a::attr(href)').extract()\n",
    "标签名 标签属性\n",
    "CSS Selector由W3C组织维护并规范"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yield关键字"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python3中33个关键字中比较重要的一个\n",
    "yield->生成器"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "生成器是一个不断产生值的函数\n",
    "包含yield语句的函数就是一个生成器\n",
    "生成器每次产生一个值（yield语句），函数被冻结，被唤醒后再产生一个值\n",
    "唤醒之后，所使用的局部变量的值和之前所使用的值是一致的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 生成器实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(n):\n",
    "    for i in range(n):\n",
    "        yield i**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  1  4  9  16  "
     ]
    }
   ],
   "source": [
    "for i in gen(5):\n",
    "    print(i,' ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 普通写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(n):\n",
    "    ls=[i**2 for i in range(n)]\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  1  4  9  16  "
     ]
    }
   ],
   "source": [
    "for i in square(5):\n",
    "    print(i,' ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为何要有生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 生成器相比一次列出所有内容的优势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "更节省存储空间\n",
    "响应更迅速\n",
    "使用更加灵活"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
